---
title: Program Low Level Optimization (CSAPP Reading Notes)
date: 2018-07-05
draft: false
---

写出一个不管在任何情况下都能够正确运行的程序是作为一个程序员的基本技能，在此之上，作为一个程序员，也应该写出一个清晰而简洁的代码，这样做不仅仅是为了自己能够在调试的时候看懂代码，也是为了在未来检查代码和今后需要修改代码的时候，其他人能哦过读懂和理解自己的代码。

<!--more-->

但是在数据量日益增大今天，很多情况下，程序运行的速度也成为了一个重要的考虑因素。如果一个程序要实时的处理视频和网络包，一个运行很慢的程序就不能够满足使用的要求。而对于一个计算任务非常大的程序，例如需要执行数日或数周的程序，其速度提升 20% 也能够造成巨大的影响，本文的目的在于探讨使用不同的类型的程序优化技术，使得程序优化的更快。

编写高效的程序需要做到以下几点：

1. 必须选择合适的算法和数据结构
2. 必须编写出编译器能够有效优化，转换成高效可执行代码的源代码。
3. 将大任务分成多个部分，并行计算。

对于第二个方面，我们应该理解编译器优化方法的局限性。对于 C 语言来说，由于存在指针运算和强制类型转换，使得编译器很难对于代码进行优化，在目前，最好的编译器也受到 *妨碍优化的因素* 的阻碍，妨碍优化的因素就是程序行为中那些 **严重依赖于执行环境** 的方面。程序员必须编写容易优化的代码，以帮助编译器。

## 优化的第一步
程序优化的第一步是消除不必要的工作，让代码尽可能有效的执行所期望的任务。这包括消除不必要的 **过程调用、条件测试** 和 **内存引用** 。这些优化不依赖于目标机器的任何具体属性。为了使代码的执行效率最高，在程序员和编译器面前都有一个目标机器的模型，指明如何执行各条代码，以及各个操作的时序特性。例如，编译器必须知道处理器的时序信息，这样才能确定如何编译乘法，是使用移位和加法的某种组合还是直接使用乘法指令。现代计算机使用了复杂的技术来处理机器级程序，并行执行许多条指令，执行的顺序还可能不同于其在程序中出现的顺序。

了解处理器的运作规律，就可以进行优化程序的第二布，利用处理器提供的指令级并行能力 (instruction-level parallelism) 能力，同时执行多条指令。

## 编译器的局限性
编译器在优化的过程中，只对于程序进行安全的优化。也就是对于程序可能遇到的所有情况下，编译器都保证优化后的程序和优化前的程序有一样的行为，这样就限制了编译器只进行安全的优化。为了理解这一点，我们来看以下两个过程：

```c
void twiddle1(long *xp, long *yp)
{
    *xp += *yp;
    *xp += *yp;
}

void twiddle2(long *xp, long *yp)
{
    *xp += 2* *yp;
}
```

这两个过程的结果看似是完全一致的，但是执行的效率不同。对于 `twiddle1()` 过程，进行了六次内存引用（读 yp，读 xp，写 xp，读 yp，读 xp，写 xp），而对于 `twiddle2()` 过程，只进行了 3 次内存引用（读 yp，读 xp，写 xp）。因此，基于编译器编译过程  `twiddle1()`，我们认为基于 `twiddle2()` 过程会产生更加有效的代码。

不过，我们应该考虑到当 `xp` 和 `yp` 相等的情况下。在这种情况下，两个过程的执行结果完全不同：

```c
void twiddle1(long *xp, long *yp)
{
    *xp += *yp; /* *xp=2* *xp */
    *xp += *yp; /* *xp=2* 2* *xp=4* *xp */
}

void twiddle2(long *xp, long *yp)
{
    *xp += 2* *yp; /* *xp=3* *xp */
}
```

结果是前者 `*xp` 的值增大为原来的 4 倍，后者为增大 3 倍。

这种两个指针可能指向同一个内存地址的情况被称为 **内存别名引用（memory aliasing）**。对于不同的情况下，编译器必须保证这种情况存在，而不对于该程序进行过程的优化。这造成了一个主要的妨碍优化的因素，也是可能严重限制编译器产生优化代码程序的一个方面。如果编译器不能确定两个指针指向同一个位置，就必须假设什么情况都有可能。

另外一个妨碍编译器优化的因素是函数调用。作为一个示例，考虑以下的两个过程：

```c
long f();

long func1(){
    return f()+f()+f()+f();
}

long func2(){
    return 4*f();
}
```
这两个过程的结果又是感觉是相同的结果，但是在 `func1()` 中，连续调用了四次 `f()`；而在 `func2()` 中，仅仅调用了一次。从以上的角度来看，过程 2 明显效率更高，编译器应该使用 `func2()` 为优化的版本。但是，考虑到以下的情况：

```c
 long f();
 long counter=0; 

 long f(){
     return counter++;
 }
```

在 `f()` 中修改了某个全局变量 counter，这就导致 `f()` 的引用次数变化会使该全局变量产生不同的值。所以，大多数编译器会保持原有代码中所有的函数调用不变，并不会优化相的代码。

如果想要优化函数调用的过程，可以使用两种方法：

* 内联函数
* 宏定义函数

这两种方法的本质是一样的，就是在代码中，将原有的函数调用初插入调用的函数代码，这样编译器就可以将其当做普通的代码进行优化。但是这两种方法都有调试的危险性，在调试的过程中，调试器将无法追踪该函数调用，也无法在函数内部设置断点。还有，在使用代码剖析的方式来评估程序的性能，用内联或者宏定义的函数调用是无法被正确剖析的。

## 程序性能的表述
这里引用度量标注 **元素的周期数（Cycles Per Element，CPE）**，作为一种表示程序性能和直到我们改进代码的方法。用时钟周期来表示，度量值表示的是执行了多少条指令，而不是时钟运行的有多快。

举个例子，在这里，我们提供两个不同的函数 `psum1` `psum2`，他们都有相同的功能，就是对于一个向量前 n 个元素求和，但是对应的采用的代码不同，通过比较两个函数的 CPE 值，我们可以理解为什么 CPE 是衡量程序性能的指标。

```c
/* 计算一个向量的 n 前序和 */
void psum1(float a[], float p[], long n)
{
    long i;
    p[0] = a[0];
    for (i = 1; i < n; i++)
        p[i] = p[i-1] + a[i];
}

void psum2(float a[]; float p[]; long n)
{
    long i;
    p[0]=a[0];
    for (i = 1; i < n-1; i+=2) {
        float mid_val = p[i-1] + a[i];
        p[i] = mid_val;
        p[i+1] = mid_val + a[i+1];
    }
    /* 对于 n 为偶数的情况 */
    if (i < n)
        p[i] = p[i-1] + a[i];
}
```

随着 n 值的变化，以上两个函数的计算所需周期也会发生变化，我们通过测试不同的 n 值的情况下的运行周期，使用最小二乘法，获得两种函数的 **周期 - n** 多项式。

* psum1: 368 + 9.0n
* psum2: 368 + 6.0n

可以看出，`psum2` 的斜率（一次项）系数要小于 `psum1`，也就是在 n 较大的情况下，`psum2` 的性能要好于 `psum1`。而这种优化的方法又被称作 **循环展开（loop unrolling）** 技术，至于为什么有效，我们可以看做通过增加每次循环的计算量，减少了测试环节的次数，计算在循环中的占比增加，这样处理器可以用更多的周期进行计算。

### 程序示例
为了说明一个抽象的程序是如何被系统得转换成为更加有效地代码的，我们将使用一个基于向量数据结构的运行示例。向量由两个内存块组成：头部和数据数组。头不是一个生命如下的结构。

```c
typedef struct {
    long len;
    data_t *data;
} vec_rec, *vec_ptr;
```

当然，我们会给这个结构分配一个 len 个 data_t 类型对象的数组，用来存放实际的向量元素。

在下面我们会给出一些常见的生成向量、访问向量和确定向量长度等基本过程。请注意在下面的向量访问程序 get_vec_element，它会对每个向量引用进行边界检查，边界检查减少了出错的机会，但是它也会减缓程序的执行。

```c
/* 创造一个特定长度的向量 */
vec_ptr new_vec(long len)
{
    /* 分配结构空间 */
    vec_ptr result = (vec_ptr) malloc(sizeof(vec_rec));
    data_t *data = NULL;
    if (!result)
        return NULL; /* 分配失败 */
    result->len = len;
    /* 分配向量空间 */
    if (len > 0) {
        data = (data_t *)calloc(len, sizeof(data_t));
        if (!data) {
            free((void *) result);
            return NULL; /* 分配失败 */
        }
    }
    /* data 指针指向空或者分配的向量 */
    result->data = data;
    return result;
}

/* 获得向量元素并将其储存在 dest *
 * 返回 0（越界）或者 1（成功）  */
int get_vec_element(vec_ptr v, long index, data_t *dest)
{
    if (index < 0 || index >= v->len)
        return 0;
    *dest = v->data[index];
    return 1;
}

/* 返回向量的长度 */
long vec_length(vec_ptr v)
{
    return v->len;
}
```

为了能够使用一种代码描述多种运算过程，我们采用了如下的代码：

```c
/* 合并运算的初始实现 */
/* 加法运算 */
#define IDENT 0
#define OP +

/* 乘法运算 */
// #define IDENT 1
// #define OP *

void combine1(vec_ptr, data_t *dest)
{
    long i;

    *dest = IDENT;
    for (i = 0; i < vec_length(v); i++) {
        data_t val;
        get_vec_element(v, i, &val);
        *dest = *dest OP val;
    }
}
```

在接下来的内容中，我们将会对代码进行一些变换，有些事有效的有些事无效的，而如何确定变换的组合也是很重要的，它们能使编译器对于代码进一步优化。在下面我们给出在 Intel Core i7 Haswell 处理器的机器上运行此求和和求积函数的 CPE 性能，这个机器又被称作参考机。

| 函数 | 方法 | 整数 CPE | 浮点数 CPE |
| :---: | :---: | :---: | :---: |
| combine1 | 抽象未优化的 | 22.68(+) 20.02(*) | 19.98(+) 20.18(*) |
| combine1 | 抽象 -O1 | 10.12(+) 10.12(*) | 10.17(+) 11.14(*) |

可以看出简单的 "-O1" 优化就可以有效提高程序的效率。

## 消除循环的低效率
在计算元素的乘积和算数和的时候，过程 `combine1` 调用函数 `vec_length` 作为 `for` 循环的测试条件。而在编译章节，我们了解了 `for` 循环在机器级程序中的翻译结果，它会在每一次循环时计算测试条件的值。另一方面，不管循环进行到哪里，测试条件的值保持不变。因此我们只需要在循环前调用一次 `vec_length`，并且在测试条件中一直使用此值即可。

```c
···
void combine2(vec_ptr, data_t *dest)
{
    long i;
    long length = vec_length(v);

    *dest = IDENT;
    for (i = 0; i < length; i++) {
        data_t val;
        get_vec_element(v, i, &val);
        *dest = *dest OP val;
    }
}
```

| 函数 | 方法 | 整数 CPE | 浮点数 CPE |
| :---: | :---: | :---: | :---: |
| combine1 | 抽象 -O1 | 10.12(+) 10.12(*) | 10.17(+) 11.14(*) |
| combine2 | 移动 vec_length | 7.02(+) 9.03(*) | 9.02(+) 11.03(*) |

这一类优化是比较常见的优化方法，被称作 **代码移动**。通常情况下，编译器会尝试进行这种优化，但是在本例中，涉及到调用函数，参见以上的内容我们知道，编译器会假设调用的函数有全局的副作用，故并不会对其进行优化。

## 减少过程调用
通过之前讲解，我们知道过程调用会产生开销，而且妨碍程序优化。在 `combine2` 程序中我们发现在循环体内，我们都是通过调用 `get_vec_element()` 来获取下一个向量元素的值。在这个调用中，我们可以看到每次都会对向量索引 i 和边界比较检查 i 合法性，而通过对于代码上下文的观察，在这个环境中，每次的引用都是合法的。

我们可以使用一个新的函数 `get_vec_start()`，这个函数获取向量的首地址，在后续计算中，直接访问地址，而不通过调用获取元素。

```c
data_t *get_vec_start(vec_ptr v)
{
    return v->data;
}
/* 通过访问矢量数据直接访问 */
void combine3(vec_ptr v,data_t *dest)
{
    long i;
    long length = vec_length(v);
    data_t *data = get_vec_start(v);

    *dest = IDENT;
    for (i = 0; i < length; i++){
        *dest = *dest OP data[i];
    }
}
```

| 函数 | 方法 | 整数 CPE | 浮点数 CPE |
| :---: | :---: | :---: | :---: |
| combine2 | 移动 vec_length | 7.02(+) 9.03(*) | 9.02(+) 11.03(*) |
| combine3 | 减少过程调用 | 7.17(+) 9.02(*) | 9.02(+) 11.04(*) |

有表可见，没有明显的提升。显然，循环中的其他操作形成了瓶颈，限制性能超过调用 `get_vec_element()`。但是这种转换带来了一些其他的好处，我们将其视作优化的一个环节。

## 消除不必要的内存引用

在以上的 `combine3()` 代码中，我们将 *dest 视作累积值的存放目标。通过检查内循环编译后的汇编代码，可以看出这个属性：

```
combine3 的内循环。data_t 是双浮点型，OP = *
dest 在 %rbx 中，data + i 在 %rdx 中，data + length 在 %rax 中

.L17:                           loop:
    vmovsd (%rbx), %xmm0            *dest -> %xmm0
    vmulsd (%rdx), %xmm0, %xmm0     data[i] * %xmm0 -> %xmm0
    vmovsd %xmm0, (%rbx)            %xmm0 -> *dest
    addq $8, %rdx                   i += 8
    cmpq %rax, %rdx                 if (!(%rdx - %rax)) set ZF
    jne .L17                        if (~ZF) loop:
```

在每次内循环中，我们都要经历 `*dest -> %xmm0` 和 `%xmm0 -> *dest` 两个访存过程，一次读取一次写入，也就是说我们计算完的结果要先写入内存后，下一次循环再进行读取到寄存器再计算。这个过程其实是不必要的，因为我们可以将数一直放在乘法寄存器中，等待计算循环完全完成后再将值写回内存，这样可以减少许多不必要的内存访问时间。

修改过的汇编代码：

```
combine3 的内循环。data_t 是双浮点型，OP = *
acc 在 %xmm0 中，data + i 在 %rdx 中，data + length 在 %rax 中
.L25:                           loop:
    vmulsd (%rdx), %xmm0, %xmm0     data[i] * %xmm0 -> %xmm0
    addq $8, %rdx                   i += 8
    cmpq %rax, %rdx                 if (!(%rdx - %rax)) set ZF
    jne .L25                        if (~ZF) loop:
```

```c
void combine4(vec_ptr v, data_t *dest)
{
    long i;
    long length = vec_length(v);
    data_t *data = get_vec_start(v);
    data_t acc = IDENT;

    for (i = 0; i < length; i++) {
        acc = acc OP data[i];
    }
    *dest = acc;
}
```

结果被积累在临时变量 acc 中，消除了每次循环中从内存读出并更新的过程。这个改变使程序的速度有了较大的提升，如下表：

| 函数 | 方法 | 整数 CPE | 浮点数 CPE |
| :---: | :---: | :---: | :---: |
| combine3 | 减少过程调用 | 7.17(+) 9.02(*) | 9.02(+) 11.04(*) |
| combine4 | 累计在临时变量中 | 1.27(+) 3.01(*) | 3.01(+) 5.01(*) |

有人觉得，编译器应该做到直接优化成为 `combine4` 中代码的格式，但是在实际的过程中，dest 指针不一定是否和数组名产生重名，故编译器并不会做出优化的措施。

在使用 '-O2' 的命令行选项编译 `combine3` 时，程序的效率提高许多，尽管还是没有 `combine4` 效率高：

| 函数 | 方法 | 整数 CPE | 浮点数 CPE |
| :---: | :---: | :---: | :---: |
| combine3 | -O1 编译 | 7.17(+) 9.02(*) | 9.02(+) 11.04(*) |
| combine3 | -O2 编译 | 1.60(+) 3.01(*) | 3.01(+) 5.01(*) |
| combine4 | 累计在临时变量中 | 1.27(+) 3.01(*) | 3.01(+) 5.01(*) |

在检查编译器产生的汇编代码时，我们发现内循环汇编代码发生了一个有趣的变化：

```
combine3 的 -O2 内循环。data_t 是双浮点型，OP = *
dest 在 %rbx 中，data + i 在 %rdx 中，data + length 在 %rax 中

.L22:                           loop:
    vmulsd (%rdx), %xmm0, %xmm0     data[i] * %xmm0 -> %xmm0
    addq $8, %rdx                   i += 8
    cmpq %rax, %rdx                 if (!(%rdx - %rax)) set ZF
    vmovsd %xmm0, (%rbx)            %xmm0 -> *dest
    jne .L22                        if (~ZF) loop:
```

通过调整写回的位置，减少了一次读取内存的操作，提高了效率，并且，在这种情况下，即使出现内存重名，也还是可以保证安全性。

## 理解现代处理器
由于大量的晶体被集成在一块芯片上，现代微处理器采用了复杂的硬件，试图使得程序的性能最大化。在代码级别上，看起来是一次执行一条指令，但是在实际的处理器中，是同时对多条指令求值的，这个现象被称作 *指令级并行*。现代处理器取得的了不起的功绩之一是：他们采用复杂而奇异的处理器结构，其中多条指令可以并行的执行，同时又呈现出一种简单的顺序执行指令的现象。

当一系列操作必须严格按照顺序执行时，就会遇到 *延迟界限*，因为在下一条指令开始之前，这条指令必须结束。当代码中的数据相关限制了处理器指令级并行执行能力时，延迟界限能够限制程序性能。*吞吐量界限* 刻画了处理器功能单元的原始计算能力。这个界限是程序性能的终极限制。

## 整体操作
在这里展示一个近期的 Intel 处理器的结构。这些处理器在工业中被称为 *超标量*，也就是说它可以在每个时钟周期执行多个操作，而且是 **乱序的**。指令的执行顺序不一定要与他们在机器级程序中的顺序一致。整个设计有两个部分：**指令控制单元（Instruction Control Unit，ICU）** 和 **执行单元（Execution Unit，EU）**。前者负责在内存中读出指令序列，并且根据这些指令序列产生一组针对程序数据的基本操作；后者执行这些操作。和我们之前研究过的 SEQ 和 PIPE 流水线相比，乱序处理器需要更大、更复杂的硬件，但是它们能够更好地达到更高的指令级并行度。

![][1]

如图所示，指令控制单元 ICU 负责从内存中读出指令，并产生一系列基本操作。然后执行单元 EU 完成这些操作，并指出分支预测是否正确。ICU 会提前很早对于当前执行的指令进行取指，这样它才有足够的时间对于其进行指令译码，并把操作发给 EU。

EU 接收实际的程序指令，并将其转换为一系列基本操作（也成微操作）。对于有复杂指令的机器，例如 x86 处理器，一条指令可以被译码成为多个操作，而指令如何被译码成为操作序列的细节，不同的机器都不同，这个信息是高度机密。但是我们并不需要知道这些底层细节，也可以优化自己的程序。

**退役单元（retirement unit）**记录正在进行的处理。退役单元控制内部寄存器的更新。指令在被译码完成后，指令的信息将会被放置在一个先进先出的队列中。而队列更新只有在以下两种条件下被完成：

* 这条信息的所有操作完成了，并且引起这条指令的预测被判断正确，对寄存器更新。
* 引起这条指令的某个分支预测点错误，该指令被清空。

这种方法导致预测的错误情况发生后，程序的状态不会发生改变。

对于功能单元，我们的 Intel Core i7 Haswell 参考机具有 8 个功能单元，每个单元的功能有：

1. 整数运算、浮点乘、整数和浮点数除法、分支
2. 证书运算、浮点加、整数乘、浮点乘
3. 加载、地址计算
4. 加载、地址计算
5. 储存
6. 整数运算
7. 整数运算、分支
8. 储存、地址运算

## 功能单元的性能
对于不同的功能单元，由于其负责不同的算数运算，故各个功能单元的性能并不相同，这些性能是通过以下的这些数值进行刻画的（单位为时钟周期）：

* **延迟（latency）**，完成运算所需的总时间。
* **发射时间（issue time）**，两个连续同类型运算之间需要的最小周期数。
* **容量（capacity）**，执行该运算的功能单元的数量。

| 运算 |  | 整数 | | | 浮点数 | |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | 延迟 | 发射 | 容量 | 延迟 | 发射 | 容量 |
| 加法 | 1 | 1 | 4 | 3 | 1 | 1 |
| 乘法 | 3 | 1 | 1 | 5 | 1 | 2 |
| 除法 | 3~30 | 3~30 | 1 | 3~15 | 3~15 | 1 |

表现发射时间的一种更常见的方法是指明这个功能单元的最大 *吞吐量*，定义为发射时间的倒数。对于一个容量为 *C*，发射时间为 *I* 的操作来说，处理器可能获得的吞吐量为每时钟周期 *C/I* 个操作，而最小的 CPE 吞吐界限就是 *I/C* 个周期。我们的参考机可以每个时钟周期执行两个浮点乘法运算。我们可以利用这个能力来提高程序的性能。根据计算，得出处理器的理论最小 CPE 参数。

| 函数 | 方法 | 整数 CPE | 浮点数 CPE |
| :---: | :---: | :---: | :---: |
| combine4 | 累计在临时变量中 | 1.27(+) 3.01(*) | 3.01(+) 5.01(*) |
| 延迟 |  | 1.00(+) 3.00(*) | 3.00(+) 5.00(*) |
| 吞吐量 |  | 0.50(+) 1.00(*) | 1.00(+) 0.50(*) |

我们可以看到，除了整数加法之外的运算都已经基本达到了延迟 CPE 的硬件界限，在接下来的内容里，我们将会着手于创建处理器操作的抽象模型，对于代码的执行过程在处理器层面进行详细探究。

#### 机器级代码到数据流图
综上所述，程序代码的数据流不能体现在处理器层面上面。我们将注意力集中在对于内循环的过程，因为对于一个巨大的 n 值，内循环的计算时间，也即是循环的 CPE 值是决定性能的主要因素。

```
combine3 的内循环。data_t 是双浮点型，OP = *
acc 在 %xmm0 中，data + i 在 %rdx 中，data + length 在 %rax 中
.L25:                           loop:
    vmulsd (%rdx), %xmm0, %xmm0     data[i] * %xmm0 -> %xmm0
    addq $8, %rdx                   i += 8
    cmpq %rax, %rdx                 if (!(%rdx - %rax)) set ZF
    jne .L25                        if (~ZF) loop:
```

对于形成循环的代码片段，我们可以将访问到的寄存器分为四类：

* 只读：这些寄存器只作为运算的源值，也可以作为计算的内存地址，在循环的过程中，其中的值保持不不变，在 `combine4` 循环中，%rax 寄存器为只读类型的。
* 只写：这些寄存器被作为数据传送操作的目的。本循环中没有此类寄存器。
* 局部：在循环内部被修改和使用。本例中的条件码寄存器就是这类寄存器，cmp 操作会修改他们，jne 操作又会使用到他们。
* 循环：对于循环来说，这些寄存器会在本次循环中修改，在下次循环中用到。本例中的 %rdx 和 %xmm0 是 `combine4` 的循环寄存器，对应变量 `data+i` 和 `acc`。

![][2]

根据汇编代码画出对应的数据流图，由于比较 cmp 和分支 jne 操作不直接影响数据流。处理器会中的指令控制单元会预测分支，使程序继续进行。

故我们循环的主要延迟来自于三个操作，mul、add、load 操作，而在这些操作之中，mul 操作的延迟最长，为 3 个时钟周期（或者 5 个时钟周期），我们将其视作 **关键路径（Critical Path）**。剩余的两个操作可以视作在 mul 操作运行时并行进行。也就是说在每次循环中，load 指令会在 mul 指令开始前就加载好数字，而 add 指令需要等待 mul 指令完成后才能进行加法的操作。可以看到，该程序的理论 CPE 界限值为 3.00 或者 5.00，也就是 mul 指令的 CPE 界限值。

## 增加并行性
可以使用增加累积标量的方式进行并行的计算，或者可以使用 Intel 处理器最新的 SSE 指令，进行 AVX 寄存器支持的向量计算方式增加并行性。（具体怎么用不介绍了，太长了）

以上两种方式都会使得程序超过 CPE 延迟界限，逼近 CPE 吞吐量界限，但是无法超越吞吐界限。

## 一些限制因素

#### 寄存器溢出
在增加并行性的过程中，我们增加了循环寄存器的使用的量，而参考机中的 xmm 和 ymm 寄存器都是只有 16 个，一旦在写程序的过程中超过了该值，就会将变量储存在临时栈中，而这样的操作会使 CPE 值略微上升，造成不必要的性能损失。

#### 分支预测和预测错误惩罚
不要过分关心可预测的分支。

**书写适合用条件传送实现的代码。**

## 理解内存性能

[1]: /img/2018-7-5-optimize-your-c-code/ooo_CPU.png
[2]: /img/2018-7-5-optimize-your-c-code/combine4_data_flow.png
