---
title: "LRTA* AND-OR ALPHA-BETA algorithms"
date: 2020-01-11T21:59:55-05:00
draft: false
---

In this post, I will write down my understanding of these three algorithms, which are intended to solve:

* Unknown but fully observable environment: LRTA\*
* Known but non-deterministic environment: AND-OR Tree Search
* Classical two-player adversary game: ALPHA-BETA Tree Search

The reason I put AND-OR and ALPHA-BETA Tree searches together is I think they are quite similiar, one is agent versus environment, one is agent versus another optimal agent.

The reason I skipped genetic algorithm is that it is just a concept, and contains too many sub algorithms that follow the same evolutionary concept. I wrote a program using GA solving TSP problem, [TSP example](https://github.com/xiahualiu/TSP_example).

<!--more-->

## Learning Real Time A\*

LRTA\* is a simple implement of a online search agent. It is suitable for an unknown environment and the environment is safe to explore.

The LRTA\* algorithm is just like the value iteration algorithm of MDP, but the difference is in value iteration algorithm, the environment is known, we initialize the utility randomly with a random value linked to any state.

But LRTA\* is able to solve the unknown environment, we cannot assign a value at first, instead, the agent can only get access to the states where it has been in. So LRTA\* use a `result` to store transitions, and a `H` to store the value of the explored states.

And there is a problem which cannot be overcome by any type of algorithms, i.e. asserting the optimality of the solution, since we can only explored limited state space, so there is not guarantee that there is no better solution than the one we found.

## AND-OR Search

AND-OR search include two functions that calling each other. It is an interesting idea.

AND nodes can be regarded as an unstable transition model, which leads to different next states. AND-OR search is a DFS search, every path will end in a failure (loop or dead) or goal.

OR nodes is the agent choice node, agent choose what to do in OR nodes. If the OR node is goal, OR search return the empty plan, and the upper OR nodes will add reflexes (if state n then do x) to that plan and the plan will eventually leads the agent to the goal.

In AND nodes, the nodes return "switch" plans, so not matter what the environment outcome is, the plan is consistent for an agent. However this switch may have a failure case.

It is a little tricky the fact that AND-OR search only need to find one goal, because OR searches exhausts all outcomes of an action, and AND searches are the choices of an agent, so if we only need a solution which can be not optimal, we just need find one possible plan.

## ALPHA-BETA Pruning/Search

Alpha-Beta search is actually a smarter version of minimax search. Every path maintains a value bound (alpha and beta) that some bad branches will not be considered and explored if they are in the bound.

Values need to be backed-up along the path. So every node on the path shares the same bound.
