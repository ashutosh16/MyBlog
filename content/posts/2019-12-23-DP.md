---
title: "A Dynamic Programming Tuturial"
date: 2019-12-23T17:59:10-05:00
draft: false
---

Well, it sounds old but for me, the true understanding of dynamic programming actually happended after I finished reading the book:

> Optimal Control Theory: An Introduction by Donald E. Kirk

However, the book used an example that is too complicated for a beginner to understand (I think, for me it is.). So I wrote this article to help those who want to take a peek in the basics of dynamic programming, I will try my best to make it easy and you will see that dynamic programming is quite simple and intuitive.

<!--more-->

## Some notations

In the rest of the article, some notations will appear more than once and I do not want to confuse any of you about the notations, what they mean.

The first two is state and policy:

Dynamic programming is suitable for solving problems that contain a series of decisions and states.

For example, if you are a postman and today your job is to deliver some packages in an area, a common postman will stick to the same route everyday or randomly drives the van. But a clever postman like you will try to find a path that can visit all the package addresses meanwhile has the shortest distance among all viable routes.

And for your information, This problem has another name, called [Traveling Salesman Problem (TSP)](https://en.wikipedia.org/wiki/Travelling_salesman_problem).

We can tear the problem into a series of decisions, i.e. which address to visit first, then, after you finish the first delivery, at the first address, you have to decide which address to visit next.

So here the states here, at first glance (not right though), are the different addresses, in other words, different outcomes of your decision. And the policy is like a GPS navigation system installed on your car, it tells you what is your next decision once you reach a state (address). So if your policy is optimal, you can follow the policy and traverse these addresses and finish the given task in the shortest path (and use less gas!). So

Policy:

$$P(state)=action$$

Policy works like a function, it accepts discrete states and outputs corresponding actions at given state.

However in the postman problem discribed above, the state is not simply address because once you made a decision, the outcome is not a simply change of your location, you also have one less package to deliver, so the state in this problem also has information of the package you are carry at the certain address. If you are at Address #1, but you only carry 1 package (1 address to go), apparently the situation differs from that you carry 10 packages (10 addresses to go), the biggest difference is your action set next is not same (you have 1 choice versus 10 choices).

So a state can be more complexed.

## Principle of Optimality

Since we have discussed state and policy, we can move on to the principle of optimality.

We can tear the problem into a series of decisions and states, right? Image we start at State #1 and reach State #n, the pricinple of optimality ensure that, if the policy is optimal, and path from the beginning state (State #1) to the State #(n-1) provided by the policy is optimal. This is simple to proof, if you can find a better path from State #1 to State #(n-1), you can simply do the same last step (State #(n-1) to State #n) and get a better result!

This may confuse you a little, I will show an example using the above postman problem. Imagine you have reached the Address #i and you only have no package to deliver (but you need to return to the post office). And you recall your history moves, "Did I make the right moves?". Well if you can think of a better waythat can lead you into the same situation you are right now, the route you chose is definitely not optimal, becase the next move clearly makes no difference on the total distance cost.

If we do the backward deduction several times (Since the path from State #1 to State #(n-1) is optimal, the path from State #1 to State #(n-2) is also optimal, and so on), then we can get a conclusion: 

>If you get an optimal path to finish the task, all the sub-paths from the begginning state to any states inside the path are optimal.

You may think, while, that is easy, we only need to find the optimal path from State #1 to State #2 and the optimal path from State #1 to State #3. Then we find State #1 to State #n.

Yes, that is the gist of the dynamic programming. However, the hidden difficulty in the process is, once you get the optimal path from State #1 to State #2, how do you get the optimal path from State #1 to State #3.

In some simple problems, like [Knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem). You can use this way to find the best result, becasue in such problems, states are heavily history related, (Forgive me for not able to think of a better description), so you can push your current state towards the ending.

Some of the problems are not simple like that, you need to search for all probably states that can reach State #3 in one move and check which one is the best, (In terms, which costs least).

So the forward programming process can be simply described in one formula:

$$ J^\ast=\min(C_h^\ast+C_i)$$

The J star is the best cost for current state, Ch star is the best history cost from beginning to the state before current state, Ci is the cost for taking action i in a given former state.

If you record all Ci's, you get the optimal path one by one.

## How to implement DP

Well, there are two ways to implement a dynamic programming algorithm for a problem, one is forward way, like I discribed above, and you can do it backward. For every iteration, you do not own a history when you do it backward, but you can evaluate it later, and just accumulate the action costs, until you reach the begginning state.

However they do not differ very much, choose one you like or according to the problem! Because you can planning the path in any direction you like, but you have to execute the actions one by one once you finish planning!

## A TSP Example
This example can be found at my github repository [here](https://github.com/xiahualiu/TSP_example). I used 3 different ways to solve the TSP problem but you only need the DP solution.

So In this example the problem is stated as:

> There are 4 cities on the map. Their locations are (20,20) (20,40) (40,40) (40,20) respectively and they are are X Y values in the cartesian system. Find the shortest tour among those cities, each city must be at least visited once and you cannot visit a city twice, except the city where you start off, you need to return to the begin city after visiting all cities.

In this problem, we can start at any arbitrary city because the tours are required to be closed circles as described, the optimal solution is same for every beginning choice.

Imagine we chose City #1 (20,20) to begin with.

The states in this problem need to carry such information:

1. **Total Distance**: How far did I go? (We need to know this while making comparsion in DP iteration)
2. **Visited City Set**: Unordered set of all cities I have visited. (Why this is needed was talked above)
3. **Current City**: Where am I now? (We need to know this before calculating the distance increments.)

So we have the structure of the state in this problem, what we need now is **evolving the intial state until the state reachs the final state**. This may sound strange to you, but you will know what I am talking about later.

I used a common notaion for our state.

$$g(current_city | visited_set)=total_distance$$

And we did the DP forward using our hand, what we get is a tree:

![DP tree][1]


[1]: /img/2019-12-23-DP/p2.png

This problem is simple, we simply did a breath first searching first, and this is enough to lead us to find out all the candidates in the DP iteration formulas (Because the states are historically related so the candidates can always be found out at the former layers). How we did the DP iteration? 

For example, following path 1-3-4-2 and we reach the state where current location is City #2 and we have visited {4,3,2,1}. And later we find a better path 1-4-3-2 which lets us reach exactly the same state and offers a better distance cost. So we update the state and overwrite the new path to the state.

Please know that this problem can be solved efficiently by BFS (not most efficiently though) does not mean every DP problems can be solved with BFS. We may can use another way to discover the candidate states, but the DP iteration process is same for any problem types.

