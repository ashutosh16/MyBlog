---
title: "A Dynamic Programming Tuturial"
date: 2019-12-23T17:59:10-05:00
draft: false
---

Well, it sounds old but for me, the true understanding of dynamic programming actually happended after I finished reading the book:

> Optimal Control Theory: An Introduction by Donald E. Kirk

However, the book used an example that is too complicated for a beginner to understand (I think, for me it is.). So I wrote this article to help those who want to take a peek in the basics of dynamic programming, I will try my best to make it easy and you will see that dynamic programming is quite simple and intuitive.

<!--more-->

## Some notations

In the rest of the article, some notations will appear more than once and I do not want to confuse any of you about the notations, what they mean.

The first two is state and policy:

Dynamic programming is suitable for solving problems that contain a series of decisions and states.

For example, if you are a postman and today your job is to deliver some packages in an area, a common postman will stick to the same route everyday or randomly drive the van. But a clever postman like you will try to find a path that can visit all the package addresses meanwhile has the shortest distance among all viable routes.

And for your information, This problem has an another name, called [Traveling Salesman Problem (TSP)](https://en.wikipedia.org/wiki/Travelling_salesman_problem).

We can tear the problem into a series of decisions, i.e. which address to visit first, then, after you finish the first delivery, at the first address, you have to decide which address to visit next.

So here the states here, at first glance (not right though), are the different addresses, in other words, different outcomes of your decision. And the policy is like a GPS navigation system installed on your car, it tells you what is your next decision once you reach a state (address). So if your policy is optimal, you can follow the policy and traverse these addresses and finish the given task in the shortest path (and use less gas!). So

Policy:

$$P(state)=action$$

Policy works like a function, it accepts discrete states and outputs corresponding actions at given state.

However in the postman problem discribed above, the state is not simply address because once you made a decision, the outcome is not a simply change of your location, you also have one less package to deliver, so the state in this problem also has information of the package you are carry at the certain address. If you are at Address #1, but you only carry 1 package (1 address to go), apparently the situation differs from that you carry 10 packages (10 addresses to go), the biggest difference is your action set next is not same (you have 1 choice versus 10 choices).

So a state can be more complexed.

## Principle of Optimality

Since we have discussed state and policy, we can move on to the principle of optimality.

We can tear the problem into a series of decisions and states, right? Image we start at State #1 and reach State #n, the pricinple of optimality ensure that, if the policy is optimal, and path from the beginning state (State #1) to the State #(n-1) provided by the policy is optimal. This is simple to proof, if you can find a better path from State #1 to State #(n-1), you can simply do the same last step (State #(n-1) to State #n) and get a better result!

This may confuse you a little, I will show an example using the above postman problem. Imagine you have reached the Address #i and you only have no package to deliver (but you need to return to the post office). And you recall your history moves, "Did I make the right moves?". Well if you can think of a better waythat can lead you into the same situation you are right now, the route you chose is definitely not optimal, becase the next move clearly makes no difference on the total distance cost.

If we do the backward deduction several times (Since the path from State #1 to State #(n-1) is optimal, the path from State #1 to State #(n-2) is also optimal, and so on), then we can get a conclusion: 

>If you get an optimal path to finish the task, all the sub-paths from the begginning state to any states inside the path are optimal.

You may think, while, that is easy, we only need to find the optimal path from State #1 to State #2 and the optimal path from State #1 to State #3. Then we find State #1 to State #n.

Yes, that is the gist of the dynamic programming. However, the hidden difficulty in the process is, once you get the optimal path from State #1 to State #2, how do you get the optimal path from State #1 to State #3.

In some simple problems, like [Knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem). You can use this way to find the best result, becasue in such problems, states are heavily history related, (Forgive me for not able to think of a better description), so you can push your current state towards the ending.

Some of the problems are not simple like that, you need to search for all probably states that can reach State #3 in one move and check which one is the best, (by terms, which costs least).

So the forward programming prcess can be simply described in one formula:

$$ J^\ast=\min\{C_h^\ast,C_i\}$$

The J star is the best cost for current state, Ch star is the best history cost from beginning to the state before current state, Ci is the cost for taking action i in a given former state.

If you record all Ci's, you get the optimal path one by one.

## How to implement DP

Well, there are two ways to implement a dynamic programming algorithm for a problem, one is forward way, like I discribed above, and you can do it backward. For every iteration, you do not own a history when you do it backward, but you can evaluate it later, and just accumulate the action costs, until you reach the begginning state.

However they do not differ very much, choose one you like or suit the problem! Because you can planning the path in any direction you like, but you have to execute the actions one by one when you finish planning!
